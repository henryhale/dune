{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e63494",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing & exploration\n",
    "In this phase, the goal is to transform raw text data into a structured, clean format suitable for machine learning, and to gain insights into the dataset's characteristics, potential biases, and important features.\n",
    "\n",
    "For this task, the following tools will be used;\n",
    "- `pandas` for data handling\n",
    "- `nltk` for natural language processing\n",
    "- `matplotlib` and `seaborn` for visualization\n",
    "\n",
    ">Unlike the normal convention of importing all necessary packages at the top, in this notebook the required packages are imported where they are first used - once. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfa33a",
   "metadata": {},
   "source": [
    "### 1.1 Load dataset\n",
    "The dataset is a CSV file with two columns: `text` - the input and `label` - the output or command class.\n",
    "\n",
    "In this section, necessary NLTK data is downloaded in preparation for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdcac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "# check if necessary nltk data exists otherwise download\n",
    "nltk_data = [\n",
    "    {\"package\": \"corpora/stopwords\", \"name\": \"stopwords\"},\n",
    "    {\"package\": \"tokenizers/punkt\", \"name\": \"punkt\"},\n",
    "    {\"package\": \"corpora/wordnet\", \"name\": \"wordnet\"},\n",
    "    {\"package\": \"corpora/omw-1.4\", \"name\": \"omw-1.4\"},\n",
    "    {\"package\": \"punkt_tab\", \"name\": \"punkt_tab\"},\n",
    "]\n",
    "\n",
    "for data in nltk_data:\n",
    "    try:\n",
    "        nltk.data.find(data['package'])\n",
    "    except LookupError:\n",
    "        nltk.download(data['name'])\n",
    "\n",
    "# load the dataset\n",
    "\n",
    "dataset_path = \"./data/dataset.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(\"[dataset]: loaded successfully\")\n",
    "    print(f\"[dataset]: initial shape: {df.shape}\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"[dataset]: file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1723b6",
   "metadata": {},
   "source": [
    "### 1.2 Explore dataset\n",
    "In this section, the distribution of the command classes is determined to help identify imbalanced classes that might require special handling (like oversampling, undersampling or class weighting). \n",
    "\n",
    "Since the dataset has equal samples for each class, there is little or no need for special handling. A class with fewer samples will be highly affected during training since the model might struggle to learn them effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[dataset]: class distribution\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ed9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=df, x=\"label\")\n",
    "plt.title('Distribution of Command Classes')\n",
    "plt.xlabel('Command Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7159a1d",
   "metadata": {},
   "source": [
    "### 1.3 Clean text with NLTK\n",
    "The model needs to focus on relevant features and so cleaning text is essential to standardize text and reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    # remove special characters and numbers to simplify vocabulary\n",
    "    text = re.sub(r'[^a-z\\s]','', text)\n",
    "    # break down text into individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords(irrelevant words like 'is', 'the', 'a')\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # reduce words to their base or root form like 'running' to 'run'\n",
    "    # this reduces vocabulary size and treat different inflections of a word as the same\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# apply preprocessing to text column\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded21a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[dataset]: text preprocessing complete')\n",
    "print(f\"original: '{df['text'].iloc[0]}'\")\n",
    "print(f\"cleaned: '{df['clean_text'].iloc[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f730d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned dataset\n",
    "clean_dataset_path = dataset_path.replace(\".csv\",\"-clean.csv\")\n",
    "# print(clean_dataset_path)\n",
    "df.to_csv(clean_dataset_path, index=None, index_label=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
